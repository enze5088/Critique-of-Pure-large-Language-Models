# Critique of Pure large Language Models


 近期LLM有价值的相关资源分享  --- 2025年3月3日

# Paper

## LLM

1. [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) 
2. [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) 
3. [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313)  #长文模型# #新架构#
4. [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905)  #高质量小模型#
5. [SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model](https://arxiv.org/abs/2502.02737)
6. [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)

### MLLM

1. An Introduction to Vision-Language Modeling
2. What’s in the Image? A Deep-Dive into the Vision of Vision Language Models

### Continue PreTrain

1. Simple and Scalable Strategies to Continually Pre-train Large Language Models
2. Continual Pre-Training of Large Language Models: How to (re)warm your model?

## Data curation

1. [DataComp-LM: In search of the next generation of training sets for language models （DCLM）](https://arxiv.org/abs/2406.11794)
2. A Survey on Data Selection for Language Models 
3. [QuRating: Selecting High-Quality Data for Training Language Models](https://arxiv.org/abs/2402.09739) 2024.02.15
4. [CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models](https://arxiv.org/abs/2410.18505)
5. [Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale](https://arxiv.org/abs/2409.17115)

## PostTrain

### Instruction tuning

1. Finetuned language models are zero-shot learners
2. Training language models to follow instructions with human feedback
3. Scaling Instruction-Finetuned Language Models
4. Infinity Instruct

## Long Context

1. **How to Train Long-Context Language Models (Effectively)**
2. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention
3. Ring Attention with Blockwise Transformers for Near-Infinite Context
4. Effective Long-Context Scaling of Foundation Models
5. LooGLE: Can Long-Context Language Models Understand Long Contexts?  #**长文**# #**评估**#
6. You Only Cache Once: Decoder-Decoder Architectures for Language Models
7. DIFFERENTIAL TRANSFORMER
8. HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS
9. Data Engineering for Scaling Language Models to 128K Context   #长文数据工程#

## Evaluation

1. MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs

## Search relevance



## ML SYS

1. Efficient Training of Large Language Models on Distributed Infrastructures: A Survey

## Other

1. Unraveling the Mystery of Scaling Laws: Part I

## 框架

### 训练框架

1. llama factory
2. openRLHF
3. verl

### 推理框架

1. sglang
2. vllm
3. TensorRT
